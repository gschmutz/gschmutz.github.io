<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dataplatform on Clarity</title><link>https://gschmutz.net/tags/dataplatform/</link><description>Recent content in Dataplatform on Clarity</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright © 2021, All rights reserved.</copyright><lastBuildDate>Sat, 06 Nov 2021 20:06:06 +0100</lastBuildDate><atom:link href="https://gschmutz.net/tags/dataplatform/index.xml" rel="self" type="application/rss+xml"/><item><title>Platys - Platform in a box</title><link>https://gschmutz.net/post/first-post/</link><pubDate>Sat, 06 Nov 2021 20:06:06 +0100</pubDate><guid>https://gschmutz.net/post/first-post/</guid><description>
&lt;h2 id="what-is-platys">What is Platys?&lt;/h2>
&lt;p>Platys is an open source tool for generating and provisioning Modern Data Platforms based on Docker and Docker Compose.
Its main use is for small-scale Data Lab projects, Proof-of-Concepts (PoC) or Proof-of-value (PoV) projects as well as trainings.
The user of platys can choose which services to use from a list of supported services and generate a fully working docker-compose.yml file including all necessary configuration files.&lt;/p>
&lt;p>This replaces our old approach, where we only had a static docker-compose.yml file with all services enabled by default. By generating the docker-compose.yml, the user has very fine-grained control on which services to include for a given platform.&lt;/p>
&lt;h2 id="how-does-platys-work">How does Platys work?&lt;/h2>
&lt;p>The folowing diagram shows the building blocks of platys and the basic flow when working with it.&lt;/p>
&lt;p>&lt;img src="https://gschmutz.net/images/2021/11/platys-overview.png" alt="Platys Overview">&lt;/p>
&lt;p>A concrete Platform is always generated based on a given Platform Stack. A platform stack defines the set of available and usable services and has a name and a version.
The diagram also shows the five necessary steps to create a new platform and use it.&lt;/p>
&lt;ol>
&lt;li>Initialise a new Platform context by specifying a Platform Stack. Optionally, a set of services to be enabled, can be specified with the init command.&lt;/li>
&lt;li>Optionally edit the config.yml to enable services and change default values of configuration settings.&lt;/li>
&lt;li>Generate the artefacts for the platform (mainly the docker-compose.yml but also some configuration files) by running the gen command.&lt;/li>
&lt;li>Run docker-compose up to start your platform.
Currently there is one Platform Stack publicly available, supporting a Modern (Analytical) Data Platforms. In the future, other platform stacks might be added.&lt;/li>
&lt;/ol>
&lt;h2 id="why-did-we-build-platys">Why did we build Platys?&lt;/h2>
&lt;p>There is no question that running workloads in containers simplifies deployment of applications. But one container is never enough, you need more than one container to implement a working solution, such as database, business logic, event broker ... and you need a Container Orchestration to manage these containers.&lt;/p>
&lt;p>Today Kubernetes is the most popular container orchestrator, but it comes with a lot of complexity. For production setups, Kubernetes is definitely one way to go. But for local, development or small-scale Proof-of-Concepts, we like to use Docker Compose, a very simple approach to container orchestration. With Compose, you use a YAML file to configure your application’s services.&lt;/p>
&lt;p>Especially as a consultant, coach, trainer, technology evangelist, you will be using different Compose setups for different environments.&lt;/p>
&lt;p>The longer you use Docker Compose, the more of these YAML files you get and to maintain them is quite a challenge:&lt;/p>
&lt;ul>
&lt;li>But how do you easily upgrade to a new version of a container, i.e. Apache Kafka?&lt;/li>
&lt;li>Do you manually have to go through all of these Compose files, which is a lot of work and prone to errors?&lt;/li>
&lt;li>What if you want to add a new service to one environment and you know that you have used it previously in another environment?&lt;/li>
&lt;li>Do you copy-paste configs from one YAML to another?&lt;/li>
&lt;li>How do you make sure that one service from another environment will work with all the configs and port settings of your other environment?
For these and some other challenges we were looking for a better solution:&lt;/li>
&lt;li>Wouldn't it be easier to generate the Docker Compose YAML, based on a simple configuration with some ON/OFF switches of all supported services?&lt;/li>
&lt;/ul>
&lt;p>Enter the world of platys...&lt;/p>
&lt;p>Platys is supported on Windows, macOS and 64-bit Linux.&lt;/p>
&lt;p>Behind the scenes, platys runs the generator (supporting given Platform Stack) as a Docker container. Therefore, you need to have Docker installed on the machine where you create a Platform. To run the Platform, you also need to have Docker Compose installed on the target machine, which can be different to the one you use for generating the platform.&lt;/p>
&lt;p>The generated platform can be provisioned either locally or in the cloud.&lt;/p>
&lt;h2 id="show-me-how-it-works">Show me how it works!&lt;/h2>
&lt;p>Make sure that you have already installed the Docker Engine, Docker Compose and the platys [1] toolset. For this demo we will create a simple platform with an Apache Kafka and Zookeeper cluster, including a service providing a user interface for Apache Kafka, so we can see that the platform is running correctly.&lt;/p>
&lt;p>Next we will present the five steps to create and use a platform.&lt;/p>
&lt;h3 id="step-1-initialize-environment">Step 1: Initialize environment&lt;/h3>
&lt;p>First create a directory, which will hold the platys configuration as well as the generated artefacts:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="ln">1&lt;/span>mkdir platys-example
&lt;span class="ln">2&lt;/span>&lt;span class="nb">cd&lt;/span> platys-example
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now let's initialise the current directory to use the Modern Data Analytics Platform Stack.
We specify the platform stack name trivadis/platys-modern-data-platform to use as well as the stack version 1.7.0 (1.7.0 is currently the latest version of this platform stack).
With the -n option we give the platform a meaningful name.
platys init -n demo-platform --stack trivadis/platys-modern-data-platform --stack-version 1.7.0 --structure flat
This generates a config.yml file, if it does not exist already, with all the services which can be configured for the platform.&lt;/p>
&lt;h3 id="step-2-configure-the-platform">Step 2: Configure the platform&lt;/h3>
&lt;p>Now we can configure the platform, using the config.yml file which has been created by the init command above.
In an editor (i.e. nano) open this configuration file.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="ln">1&lt;/span>nano config.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can see the configuration options, available through this platform stack, similar to this (only showing the first few lines)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="ln"> 1&lt;/span> # Default values for the generator
&lt;span class="ln"> 2&lt;/span> # this file can be used as a template for a custom configuration
&lt;span class="ln"> 3&lt;/span> # or to know about the different variables available for the generator
&lt;span class="ln"> 4&lt;/span> platys:
&lt;span class="ln"> 5&lt;/span> platform-name: &amp;#39;kafka-platform&amp;#39;
&lt;span class="ln"> 6&lt;/span> platform-stack: &amp;#39;trivadis/platys-modern-data-platform&amp;#39;
&lt;span class="ln"> 7&lt;/span> platform-stack-version: &amp;#39;1.7.0&amp;#39;
&lt;span class="ln"> 8&lt;/span> structure: &amp;#39;flat&amp;#39;
&lt;span class="ln"> 9&lt;/span>
&lt;span class="ln">10&lt;/span> # ===== Apache Zookeeper ========
&lt;span class="ln">11&lt;/span> KAFKA_enable: true
&lt;span class="ln">12&lt;/span> # one of enterprise, community
&lt;span class="ln">13&lt;/span> KAFKA_edition: &amp;#39;community&amp;#39;
&lt;span class="ln">14&lt;/span> KAFKA_volume_map_data: false
&lt;span class="ln">15&lt;/span> KAFKA_broker_nodes: 3
&lt;span class="ln">16&lt;/span> KAFKA_delete_topic_enable: false
&lt;span class="ln">17&lt;/span> KAFKA_auto_create_topics_enable: false
&lt;span class="ln">18&lt;/span>
&lt;span class="ln">19&lt;/span> ...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can now enable the options for the services you want the platform to support by changing false to true.&lt;/p>
&lt;p>For enabling Kafka and Zookeeper, all we have to do is set the ZOOKEEPER_enable and KAFKA_enable flag to true. Additionally, we also enable the Apache Kafka HQ User Interface, so that we are able to see that Kafka is running in step 4.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="ln"> 1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">#zookeeper&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 2&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ZOOKEEPER_enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 3&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ZOOKEEPER_volume_map_data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 4&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ZOOKEEPER_nodes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># either 1 or 3&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 5&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 6&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">#kafka&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 7&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">KAFKA_enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 8&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">KAFKA_entreprise_enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln"> 9&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">KAFKA_volume_map_data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln">10&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">KAFKA_broker_nodes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln">11&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">KAFKA_delete_topic_enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln">12&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">KAFKA_auto_create_topics_enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln">13&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln">14&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">...&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="ln">15&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">AKHQ_enable&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You only have to explicitly enable what you need, as each service is disabled by default. Other settings have meaningful defaults as well. So you can also remove the services you don't need.&lt;/p>
&lt;h3 id="step-3-generate-the-platform">Step 3: Generate the platform&lt;/h3>
&lt;p>Now we are ready to generate the platform. In the platys-example folder, run the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="ln">1&lt;/span>platys gen
&lt;/code>&lt;/pre>&lt;/div>&lt;p>and you should see an output similar to this&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="ln"> 1&lt;/span>Running the Modern Data Platform Stack Generator ....
&lt;span class="ln"> 2&lt;/span>Destination = /home/bigdata/mdps-stack-test
&lt;span class="ln"> 3&lt;/span>
&lt;span class="ln"> 4&lt;/span>Process Definition: &amp;#39;/opt/mdps-gen/stack-config.yml&amp;#39;
&lt;span class="ln"> 5&lt;/span>Loading file &amp;#39;/opt/mdps-gen/stack-config.yml&amp;#39;...
&lt;span class="ln"> 6&lt;/span>Parsing YAML...
&lt;span class="ln"> 7&lt;/span>Loading file &amp;#39;/opt/mdps-gen/vars/default-values.yml&amp;#39;...
&lt;span class="ln"> 8&lt;/span>Parsing YAML...
&lt;span class="ln"> 9&lt;/span>Loading file &amp;#39;/tmp/custom-stack-config.yml&amp;#39;...
&lt;span class="ln">10&lt;/span>Parsing YAML...
&lt;span class="ln">11&lt;/span>Return cached file &amp;#39;/opt/mdps-gen/vars/default-values.yml&amp;#39;...
&lt;span class="ln">12&lt;/span>Parsing YAML...
&lt;span class="ln">13&lt;/span>Return cached file &amp;#39;/tmp/custom-stack-config.yml&amp;#39;...
&lt;span class="ln">14&lt;/span>Parsing YAML...
&lt;span class="ln">15&lt;/span>Render template: &amp;#39;templates/docker-compose.yml.j2&amp;#39; --&amp;gt; &amp;#39;destination/docker-compose.yml&amp;#39;
&lt;span class="ln">16&lt;/span>Loading file &amp;#39;/opt/mdps-gen/templates/docker-compose.yml.j2&amp;#39;...
&lt;span class="ln">17&lt;/span>Parsing YAML...
&lt;span class="ln">18&lt;/span>Dumping YAML...
&lt;span class="ln">19&lt;/span>Writing file &amp;#39;/opt/mdps-gen/destination/docker-compose.yml&amp;#39;...
&lt;span class="ln">20&lt;/span>Render template: &amp;#39;templates/mdps-services.yml.j2&amp;#39; --&amp;gt; &amp;#39;destination/mdps-services.yml&amp;#39;
&lt;span class="ln">21&lt;/span>Loading file &amp;#39;/opt/mdps-gen/templates/mdps-services.yml.j2&amp;#39;...
&lt;span class="ln">22&lt;/span>Parsing YAML...
&lt;span class="ln">23&lt;/span>Dumping YAML...
&lt;span class="ln">24&lt;/span>Writing file &amp;#39;/opt/mdps-gen/destination/mdps-services.yml&amp;#39;...
&lt;span class="ln">25&lt;/span>Modern Data Platform Stack generated successfully to /home/docker/Desktop/kafka-plaform-example
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You now find a fully configured docker-compose.yml file (with the services enabled in the config.yml) as well as some static configuration files, necessary for the services.&lt;/p>
&lt;h3 id="step-4-run-the-platform">Step 4: Run the platform&lt;/h3>
&lt;p>Now the Platform is ready to be started. Before doing that, you have to create some environment variables, depending on the services you use. In minimum you should create&lt;/p>
&lt;ul>
&lt;li>&lt;code>DOCKER_HOST_IP&lt;/code> - the IP address of the network interface of the Docker Host&lt;/li>
&lt;li>&lt;code>PUBLIC_IP&lt;/code> - the IP address of the public network interface of the Docker Host (different to DOCKER_HOST_IP if in a public cloud environment&lt;/li>
&lt;/ul>
&lt;p>You can set these environment variables persistently on the machine (&lt;code>/etc/environment&lt;/code>) or user (&lt;code>~/.pam_environment&lt;/code> or &lt;code>~/.profile&lt;/code>) level. Another option is to use the .env file in the folder where the docker-compose.yml file is located. All environment variables set in there are used when the docker compose environment is started.&lt;/p>
&lt;p>Now let's start the platform. In a terminal window, execute&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="ln">1&lt;/span>docker-compose up -d
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Docker will start downloading the necessary container images and then start the platform.&lt;/p>
&lt;p>To see the logs of all the services, perform&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="ln">1&lt;/span>docker-compose logs -f
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can list a number of service to only see a log for them
&lt;code>docker-compose logs -f &amp;lt;service-name&amp;gt; &amp;lt;service-name&amp;gt;&lt;/code>&lt;/p>
&lt;p>After a short while all the services should start up and the platform is ready to be used.&lt;/p>
&lt;h3 id="step-5-use-the-platform">Step 5: Use the Platform&lt;/h3>
&lt;p>After a short while, the platform with Apache Kafka and Zookeeper should be up and running and ready to use. We can make sure that this is the case by either checking the log file or by navigating to the Apache Kafka HQ service, which provides a nice browser-based view into the Apache Kafka cluster.&lt;/p>
&lt;p>In a browser, navigate to &lt;a href="http://dataplatform:80">http://dataplatform:80&lt;/a>.&lt;/p>
&lt;p>In a browser window, navigate to &lt;code>http://&amp;lt;public-ip&amp;gt;:28107/ui/docker-kafka-server/node&lt;/code>, replacing &lt;public-ip> by the IP address of the public network interface of the Docker Host. You should see an output similar to the one shown in Figure 2, showing the 3 Kafka brokers of the Apache Kafka cluster we are running on the platform.&lt;/p>
&lt;p>&lt;img src="https://gschmutz.net/images/2021/11/platys-overview.png" alt="AKHQ Homepage">&lt;/p>
&lt;p>The Kafka cluster is ready, and you can start creating Kafka topics, and publish and consume messages.&lt;/p>
&lt;p>If you are finished using the platform, you can stop and remove it using the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="ln">1&lt;/span>docker-compose down
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>: be aware that this completely removes the Docker containers and by that all the data within it. If you haven't mapped the data outside the container, then you might lose your work!&lt;/p>
&lt;p>At this point, you have seen the basics of how platys works using the modern-data-platform stack.&lt;/p></description></item></channel></rss>